apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-config
  namespace: airflow
data:
  # Core Airflow settings
  AIRFLOW__CORE__EXECUTOR: CeleryExecutor
  AIRFLOW__CORE__LOAD_EXAMPLES: "False"
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "False"
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "False"
  AIRFLOW__CORE__PARALLELISM: "32"
  AIRFLOW__CORE__DAG_CONCURRENCY: "16"
  AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: "16"
  AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "300"
  
  # Celery settings
  AIRFLOW__CELERY__WORKER_CONCURRENCY: "4"
  AIRFLOW__CELERY__WORKER_PREFETCH_MULTIPLIER: "1"
  
  # Logging - update with your cloud storage path
  # AWS: s3://my-bucket/airflow-logs
  # GCP: gs://my-bucket/airflow-logs
  # Azure: wasbs://container@account.blob.core.windows.net/airflow-logs
  AIRFLOW__LOGGING__REMOTE_LOGGING: "False"
  # AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "s3://my-bucket/airflow-logs"
  
  # Database
  # Update with your managed database endpoint
  # AWS RDS: postgresql://user:pass@airflow-db.xxxxx.rds.amazonaws.com:5432/airflow
  # GCP Cloud SQL: postgresql://user:pass@127.0.0.1:5432/airflow (via cloud-sql-proxy)
  # Azure: postgresql://user@server:pass@server.postgres.database.azure.com:5432/airflow
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql://airflow:password@postgres.default:5432/airflow"
  
  # Celery broker & result backend
  # Update with your managed Redis endpoint
  # Format: redis://hostname:6379/db
  AIRFLOW__CELERY__BROKER_URL: "redis://redis.default:6379/0"
  AIRFLOW__CELERY__RESULT_BACKEND: "redis://redis.default:6379/1"
  
  # Webserver
  AIRFLOW__WEBSERVER__BASE_LOG_FOLDER: "/opt/airflow/logs"
  AIRFLOW__WEBSERVER__WEB_SERVER_PORT: "8080"
  AIRFLOW__WEBSERVER__WORKERS: "4"
  AIRFLOW__WEBSERVER__WORKER_CLASS: "sync"
  AIRFLOW__WEBSERVER__WORKER_TIMEOUT: "300"
  
  # Scheduler
  AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: "False"
  AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "True"
