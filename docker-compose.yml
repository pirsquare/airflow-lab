version: '3.8'

services:
  redis:
    image: redis:7-alpine
    container_name: airflow-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - airflow
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: redis-server --appendonly yes

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow-init
    env_file:
      - .env
    command: >
      bash -c "
      airflow db migrate && \
      airflow users create \
        --username ${AIRFLOW_ADMIN_USER:-admin} \
        --password ${AIRFLOW_ADMIN_PASSWORD:-admin} \
        --firstname Admin \
        --lastname User \
        --role Admin \
        --email ${AIRFLOW_ADMIN_EMAIL:-admin@example.com} \
        || true && \
      echo '---' && \
      echo 'Airflow initialized successfully!' && \
      echo 'WebUI: http://localhost:8080' && \
      echo 'Username: ${AIRFLOW_ADMIN_USER:-admin}' && \
      echo 'Password: ${AIRFLOW_ADMIN_PASSWORD:-admin}' && \
      echo '---'
      "
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://redis:6379/1
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=False
      - AIRFLOW_ADMIN_USER=${AIRFLOW_ADMIN_USER:-admin}
      - AIRFLOW_ADMIN_PASSWORD=${AIRFLOW_ADMIN_PASSWORD:-admin}
      - AIRFLOW_ADMIN_EMAIL=${AIRFLOW_ADMIN_EMAIL:-admin@example.com}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./plugins:/opt/airflow/plugins
      - ./logs:/opt/airflow/logs
      - airflow_data:/opt/airflow
    networks:
      - airflow
    depends_on:
      redis:
        condition: service_healthy

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow-webserver
    env_file:
      - .env
    command: webserver
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://redis:6379/1
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=False
    volumes:
      - ./dags:/opt/airflow/dags
      - ./plugins:/opt/airflow/plugins
      - ./logs:/opt/airflow/logs
      - airflow_data:/opt/airflow
    networks:
      - airflow
    depends_on:
      - redis
      - airflow-init
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow-scheduler
    env_file:
      - .env
    command: scheduler
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://redis:6379/1
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=False
    volumes:
      - ./dags:/opt/airflow/dags
      - ./plugins:/opt/airflow/plugins
      - ./logs:/opt/airflow/logs
      - airflow_data:/opt/airflow
    networks:
      - airflow
    depends_on:
      - redis
      - airflow-init

  airflow-worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow-worker
    env_file:
      - .env
    command: celery worker
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://redis:6379/1
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=False
    volumes:
      - ./dags:/opt/airflow/dags
      - ./plugins:/opt/airflow/plugins
      - ./logs:/opt/airflow/logs
      - airflow_data:/opt/airflow
    networks:
      - airflow
    depends_on:
      - redis
      - airflow-init

  airflow-triggerer:
    env_file:
      - .env
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow-triggerer
    command: triggerer
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://redis:6379/1
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=False
    volumes:
      - ./dags:/opt/airflow/dags
      - ./plugins:/opt/airflow/plugins
      - ./logs:/opt/airflow/logs
      - airflow_data:/opt/airflow
    networks:
      - airflow
    depends_on:
      - redis
      - airflow-init

volumes:
  redis_data:
  airflow_data:

networks:
  airflow:
    driver: bridge
